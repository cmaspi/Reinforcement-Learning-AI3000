{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - a\n",
    "The states are as follows\n",
    "$$\\{S,1,7,3,8,5,6,W\\}$$\n",
    "This is because we can club together equivalent states, which are \n",
    "$$\\{2 \\equiv 7\\}, \\{3 \\equiv 9\\}, \\{4 \\equiv 8\\}$$\n",
    "\n",
    "The State transition matrix is stated below\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "                  0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0.25 & 0 & 0.25 & 0.25 & 0.25 & 0 \\\\\n",
    "                  0 & 0 & 0 & 0.25 & 0.5 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0.25 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "Note that the states in transition matrix are in the order of states written above.\n",
    "\n",
    "### Part-b\n",
    "The reward function would be \n",
    "$$\n",
    "R(s) = \\begin{cases}\n",
    "                        0 & \\text{if } s = W \\\\\n",
    "                        -1 & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "$$\n",
    "In vector form it could be represented as\n",
    "$$\n",
    "R = \\begin{pmatrix}\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                0\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "The discount factor should be 1 because our goal is to find the expected number of moves.\n",
    "\n",
    "*Why?* The reward should be -1 for all the states except for the winning state because our goal is to find the average number of steps to reach the goal state, while our discount factor should be 1 again for the same reason.\n",
    "\n",
    "Now we can use the matrix form derived from the Bellman equation to solve for the value vector\n",
    "\n",
    "$$ V = \\left( I - \\gamma P \\right)^{-1}R $$\n",
    "\n",
    "The Calculation can be found in the cell below.\n",
    "\n",
    "The expected number of die throws should be slightly above 7.08 from the starting state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why can we make the probability of transition from absorbing state back to itself 0?\n",
    "The value function evaluated at the absorbing state is just the reward at that state, if there are $N$ states and $1$ absorbing states. We have already evaluated the value function at the absorbing state, now to find the value at other states we set the probability of transition from absorbing state back to itself to 0. This helps reducing the equations to $\\left(N-1\\right)$ in $V = R + \\gamma P V$ and also accounts for the value at absorbing state (which is same as the reward at that state).\n",
    "**How does it solve the problem of singularity?**   \n",
    "$$V = R + \\gamma P V$$\n",
    "The above equation is derived from \n",
    "$$ V(s) = \\text{E}\\left[r_{t+1}|s_t=s\\right]+\\gamma\\sum_{s'\\in S}P_{ss'}V(s')$$\n",
    "\n",
    "Notice that when $s$ is the absorbing state, the summation on the right becomes 0, because we won't evaluate it any further instead of defining the Value recursively as a function of value at absorbing state itself. This problem can be avoided by setting $P_{ss}$ to 0 while solving the Bellman equation where $s$ is the absorbing state.\n",
    "\n",
    "We can consider an example.     \n",
    "Consider a two state MRP with states $\\{A, B\\}$, the transition matrix being\n",
    "$$\n",
    "\\begin{bmatrix}0.1 & 0.9 \\\\ 0 & 1\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "If we write the equation for value functions we get\n",
    "\\begin{align}\n",
    "V(A) &= R(A) + 0.1\\times V(A) + 0.9 \\times V(B)\\\\\n",
    "V(B) &= R(B)\n",
    "\\end{align}\n",
    "This is why we need to make the transition probability from B to B 0, while using the matrix approach. If we don't the matrix approach would be equivalent to\n",
    "\\begin{align}\n",
    "V(A) &= R(A) + 0.1\\times V(A) + 0.9 \\times V(B)\\\\\n",
    "V(B) &= R(B) + V(B)\n",
    "\\end{align}\n",
    "which is wrong and is the cause of invertibility, this would happend in any finite horizon MRP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected number of moves from each state is: [ 7.08333333  7.          5.33333333  6.66666667  5.33333333  6.66666667\n",
      "  5.33333333 -0.        ]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "# Reward Vector\n",
    "R = -np.ones(8)\n",
    "R[-1] = 0\n",
    "\n",
    "# transition Matrix\n",
    "P = np.array(\n",
    "            [[0 , 0.25 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0.25 , 0 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0.25 , 0 , 0.25 , 0.25 , 0.25 , 0],\n",
    "             [0 , 0 , 0 , 0.25 , 0.5 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0.25 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0 , 0 , 0 , 0 , 0 , 0]]\n",
    "            )\n",
    "\n",
    "def ValueVector(P: np.ndarray, R: np.ndarray, gamma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given transition matrix, reward vector and discount factor this\n",
    "    function calculates the value vector\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        P (np.ndarray): transition matrix\n",
    "        R (np.ndarray): Reward vector\n",
    "        gamma (int): the discount factor\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        np.ndarray: the value vector\n",
    "    \"\"\"\n",
    "    # The identity matrix\n",
    "    I = np.eye(*P.shape)\n",
    "    return scipy.linalg.inv((I - gamma*P)).dot(R)\n",
    "\n",
    "V = ValueVector(P, R, 1.0)\n",
    "print(f'The expected number of moves from each state is: {-V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "# Question-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Space: It can be denoted by the number of machines that are working.\n",
    "\n",
    "$$ S := \\{0, 1 ,\\dots, N\\} $$\n",
    "\n",
    "Action Space: \n",
    "- It is either don't do anything,\n",
    "- or get the machines repaired\n",
    "\n",
    "Lets say action $a_1$ is to not to anything, while action $a_2$ is to get the machines repaired.\n",
    "\n",
    "Rewards: It has two components -\n",
    "- k if total of k machines are working\n",
    "- $-N/2$ if repair is done else 0\n",
    "\n",
    "$$R(S_{t+1} = S|S_t=k, a_t = a_1) = k$$\n",
    "$$R(S_{t+1} = S|S_t=k, a_t = a_2) = k- N/2$$\n",
    "\n",
    "The transition Matrix \n",
    "$$P^{a_1} = $$\n",
    "| $m$ | $n=0$ | $n=1$| $\\dots$ | $n=N$ |\n",
    "| --- | ---- | ---- | ---- | ---- |\n",
    "|m=0| $1$ | 0 | $\\dots$ | 0 |\n",
    "|m=1| $\\frac{1}{2}$ | $\\frac{1}{2}$ | $\\dots$ | 0 |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$ |\n",
    "| $m = N$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$| \n",
    "\n",
    "\n",
    "The transition Matrix \n",
    "$$P^{a_2} = $$\n",
    "| $m$ | $n=0$ | $n=1$| $\\dots$ | $n=N-1$ |$n=N$ |\n",
    "| --- | ---- | ---- | ---- | ---- | ---- |\n",
    "|m=0| $0$ | 0 | $\\dots$ | 0 | 1 | \n",
    "|m=1| 0 | 0 | $\\dots$ | $\\frac{1}{2}$ | $\\frac{1}{2}$ |\n",
    "| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\ddots$ | $\\vdots$ | $\\vdots$ |\n",
    "| $m = N$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$ | $\\frac{1}{N+1}$| $\\frac{1}{N+1}$|\n",
    "\n",
    "The reward matrix is given by\n",
    "$$ R^{a_1} = \\begin{bmatrix}\n",
    "                0 & 0 & \\dots & 0 \\\\\n",
    "                1 & 1 & \\dots & 1 \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                N-1 & N-1 & \\dots & N-1 \\\\\n",
    "                N & N & \\dots & N \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ R^{a_2} = \\begin{bmatrix}\n",
    "                -\\frac{N}{2} & -\\frac{N}{2} & \\dots & -\\frac{N}{2}\\\\\n",
    "                1-\\frac{N}{2} & 1-\\frac{N}{2} & \\dots & 1-\\frac{N}{2} \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                N-1-\\frac{N}{2} & N-1-\\frac{N}{2} & \\dots & N-1-\\frac{N}{2} \\\\\n",
    "                N-\\frac{N}{2} & N-\\frac{N}{2} & \\dots & N-\\frac{N}{2} \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Part-b\n",
    "The given setting doesn't have an absorbing state, it is an infinite horizon MDP, which means we should use a discounted setting.\n",
    "\n",
    "## Part-c\n",
    "Now, we would follow a policy where only action $a_1$ is taken. \n",
    "\n",
    "The state space is given by: $\\{0, 1, 2, 3, 4, 5\\}$ which represents the number of machines working.\n",
    "\n",
    "The transition matrix is given by\n",
    "\n",
    "$$ P^{\\pi} = \\begin{bmatrix}\n",
    "               1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "               \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0 \\\\\n",
    "               \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 & 0 \\\\\n",
    "               \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 & 0 \\\\\n",
    "               \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & 0 \\\\\n",
    "               \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} & \\frac{1}{6} \\\\\n",
    "       \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ R^{\\pi}(s) = \\sum \\pi(a|s) \\sum_{s'} P^{a}_{SS'}R^{a}_{SS'}$$\n",
    "\n",
    "Where $\\pi(a|s)$ is always 1, as the probability of taking action $a_1$ is 1.\n",
    "\n",
    "$$ R^{\\pi}(s) = \\sum_{s'} P^{a_1}_{SS'}R^{a_1}_{SS'}$$\n",
    "\n",
    "The reward matrix is given as\n",
    "\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "         0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "         1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "         2 & 2 & 2 & 2 & 2 & 2 \\\\\n",
    "         3 & 3 & 3 & 3 & 3 & 3 \\\\\n",
    "         4 & 4 & 4 & 4 & 4 & 4 \\\\\n",
    "         5 & 5 & 5 & 5 & 5 & 5\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can either calculate the Reward vector using the equation written above, or we can directly say that the reward for being in state S is same as the number of machines that are correctly working on that day.\n",
    "\n",
    "$$\n",
    "R^{\\pi} = \\begin{pmatrix}\n",
    "            0 \\\\ 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\n",
    "          \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value vector is [ 0.  2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "def generate_subarray(how_many_div: int, size: int) -> np.ndarray:\n",
    "    res = np.zeros(size)\n",
    "    for i in range(how_many_div):\n",
    "        res[i] = 1/how_many_div\n",
    "    return res\n",
    "# Now we have a finite horizon MDP, so we can use discount factor as 1\n",
    "\n",
    "P = np.array([generate_subarray(i+1, 6) for i in range(6)])\n",
    "P[0, 0] = 0\n",
    "\n",
    "R = np.array([0, 1, 2, 3, 4, 5])\n",
    "gamma = 1\n",
    "\n",
    "V = ValueVector(P, R, gamma)\n",
    "print(f\"The value vector is {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi'(s) = greedy(V)$$\n",
    "\n",
    "$$\n",
    "\\pi'(s) = \\begin{cases}\n",
    "                1 & \\text{if } a = \\text{argmax}_{a \\in A} \\displaystyle  \\left[\\sum_{s' \\in S} P_{ss'}^a (R_{ss'}^a+\\gamma V^\\pi(s'))\\right] \\\\ \n",
    "                0 & \\text{otherwise}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "Now, lets perform the iteration for each state, we would compare the Value inside the summation for each action, lets call the value as Q\n",
    "1. for $s = 0$\n",
    "    - Action $a_1$: $Q(0, a_1)$ = $0$\n",
    "    - Action $a_2$: $Q(0, a_2)$ = $7.5$   \n",
    "    Take Action $a_2$\n",
    "2. For $s = 1$\n",
    "    - Action $a_1$: $Q(1, a_1)$ = $2$\n",
    "    - Action $a_2$: $Q(1, a_2)$ = $7.5$   \n",
    "    Take Action $a_2$\n",
    "3. For $s = 2$\n",
    "    - Action $a_1$: $Q(2, a_1)$ = $4$\n",
    "    - Action $a_2$: $Q(2, a_2)$ = $7.5$   \n",
    "    Take Action $a_2$\n",
    "4. For $s = 3$\n",
    "    - Action $a_1$: $Q(3, a_1)$ = $6$\n",
    "    - Action $a_2$: $Q(3, a_2)$ = $7.5$   \n",
    "    Take Action $a_2$\n",
    "4. For $s = 4$\n",
    "    - Action $a_1$: $Q(4, a_1)$ = $8$\n",
    "    - Action $a_2$: $Q(4, a_2)$ = $7.5$   \n",
    "    Take Action $a_1$\n",
    "5. For $s = 5$\n",
    "    - Action $a_1$: $Q(5, a_1)$ = $10$\n",
    "    - Action $a_2$: $Q(5, a_2)$ = $7.5$   \n",
    "    Take Action $a_1$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "# Question-3\n",
    "State Space S := $\\{A, B, C, D\\}$\n",
    "\n",
    "### Policy $\\pi_1$\n",
    "$$P^{\\pi_1}(S'|S) = \\sum_{a \\in A} \\pi(a|s) P_{SS'}^a$$\n",
    "\n",
    "$$\n",
    "P^{\\pi_1} = \\begin{bmatrix}\n",
    "                        0 & 0.9 & 0.1 & 0 \\\\\n",
    "                        0.1 & 0 & 0 & 0.9 \\\\\n",
    "                        0.9 & 0 & 0 & 0.1 \\\\\n",
    "                        0 & 0 & 0 & 1 \\\\\n",
    "            \\end{bmatrix}                          \n",
    "$$\n",
    "\n",
    "$$R^{\\pi_1}(s) = \\sum \\pi_1(a|s) \\sum_{s'}P_{ss'}^aR_{ss'}^a$$\n",
    "\n",
    "We aren't given the rewards in such a format, but for every state apart from the goal state (D) the reward is $-10$, while for the goal state it is $100$\n",
    "\n",
    "$$R^{\\pi_1} = \\begin{pmatrix} -10 \\\\ -10 \\\\ -10 \\\\ 100 \\end{pmatrix}$$\n",
    "\n",
    "### Policy $\\pi_2$\n",
    "$$P^{\\pi_2}(S'|S) = \\sum_{a \\in A} \\pi(a|s) P_{SS'}^a$$\n",
    "\n",
    "$$\n",
    "P^{\\pi_2} = \\begin{bmatrix}\n",
    "                        0 & 0.1 & 0.9 & 0 \\\\\n",
    "                        0.9 & 0 & 0 & 0.1 \\\\\n",
    "                        0.1 & 0 & 0 & 0.9 \\\\\n",
    "                        0 & 0 & 0 & 1 \\\\\n",
    "            \\end{bmatrix}                            \n",
    "$$\n",
    "\n",
    "The reward vector would remain that same that is\n",
    "$$\n",
    "R^{\\pi_2}(s) = \\sum \\pi_2(a|s) \\sum_{s'}P_{ss'}^aR_{ss'}^a\n",
    "$$\n",
    "We get\n",
    "$$\n",
    "R^{\\pi_2} = \\begin{pmatrix} -10 \\\\ -10 \\\\ -10 \\\\ 100 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Policy $\\pi_3$\n",
    "- Action $a_1$ is chosen in states B and D with probability $1.0$\n",
    "- Action $a_2$ is chosen in state C with probability $1.0$\n",
    "- Action $a_1$ is chosen in state A with probability $0.4$ and action $a_2$ is chosen with probability $0.6$\n",
    "\n",
    "$$P^{\\pi_3}(S'|S) = \\sum_{a \\in A} \\pi(a|s) P_{SS'}^a$$\n",
    "$$\n",
    "P^{\\pi_3} = \\begin{bmatrix}\n",
    "                        0 & 0.42 & 0.58 & 0 \\\\\n",
    "                        0.1 & 0 & 0 & 0.9 \\\\\n",
    "                        0.1 & 0 & 0 & 0.9 \\\\\n",
    "                        0 & 0 & 0 & 1 \\\\\n",
    "            \\end{bmatrix}                          \n",
    "$$\n",
    "\n",
    "The reward vector would remain that same that is\n",
    "$$\n",
    "R^{\\pi_3}(s) = \\sum \\pi_2(a|s) \\sum_{s'}P_{ss'}^aR_{ss'}^a\n",
    "$$\n",
    "We get\n",
    "$$\n",
    "R^{\\pi_3} = \\begin{pmatrix} -10 \\\\ -10 \\\\ -10 \\\\ 100 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy 1\n",
    "P1 = np.array([\n",
    "               [0, 0.9, 0.1, 0],\n",
    "               [0.1, 0, 0, 0.9],\n",
    "               [0.9, 0, 0, 0.1],\n",
    "               [0 ,0 ,0, 0]\n",
    "              ])\n",
    "R1 = np.array([-10, -10, -10, 100])\n",
    "gamma = 1\n",
    "\n",
    "V1 = ValueVector(P1, R1, 1)\n",
    "\n",
    "# Policy 2\n",
    "P2 = np.array([\n",
    "               [0, 0.1, 0.9, 0],\n",
    "               [0.9, 0, 0, 0.1],\n",
    "               [0.1, 0, 0, 0.9],\n",
    "               [0 ,0 ,0, 0]\n",
    "              ])\n",
    "R2 = R1\n",
    "\n",
    "V2 = ValueVector(P2, R2, 1)\n",
    "\n",
    "# Policy 3\n",
    "P3 = np.array([\n",
    "               [0, 0.42, 0.58, 0],\n",
    "               [0.1, 0, 0, 0.9],\n",
    "               [0.1, 0, 0, 0.9],\n",
    "               [0 ,0 ,0, 0]\n",
    "              ])\n",
    "R3 = R1\n",
    "\n",
    "V3 = ValueVector(P3, R3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.) The value vectors for all the policies are\n",
      "V1: [ 75.6097561   87.56097561  68.04878049 100.        ]\n",
      "V2: [ 75.6097561   68.04878049  87.56097561 100.        ]\n",
      "V3: [ 77.77777778  87.77777778  87.77777778 100.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"a.) The value vectors for all the policies are\")\n",
    "print(f'V1: {V1}\\nV2: {V2}\\nV3: {V3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.)** The policy $\\pi_3$ is the best policy among the given policies, because the $\\pi_3 \\geq \\pi_1$ and $\\pi_3 \\geq \\pi_2$ (partial ordering).\n",
    "\n",
    "**c.)** The policies $\\pi_1$ and $\\pi_2$ are not comparable since we are not given a starting state and we cannot use poset since the value at B is higher in policy $\\pi_1$ while at C is higher in policy $\\pi_2$. The policy $\\pi_3$ is better than both of the other policies.\n",
    "\n",
    "**d.)** We can use a greedy approach. Basically for each state look which policy gives a higher value function at that state. Choose that policy. \n",
    "\n",
    "$$\\pi(s) = \\begin{cases}\n",
    "\\pi_1(s) & \\text{if } V^{\\pi_1}(s) > V^{\\pi_2}(s) \\\\\n",
    "\\pi_2(s) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "This ensures that at each state the value function evaluated at that state would be at least as high as the max of the value functions evaluated using the given two policies.\n",
    "\n",
    "Essentially, we are choosing the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Question-4\n",
    "### Case #1: $\\gamma = 0.1$, $\\eta = 0$ (low discount factor, low noise)\n",
    "When the discount factor is small, we are giving preference to the immediate rewards. We are also assuming that there is no noise in the environment. The agent would prefer the close exit that is the state with **$+1$** reward because the discount factor is rather low, achieving the state with **$+10$** would take two more moves and when discounted with $0.1$ it would give an overall lower incentive. Therefore, for low discount factor $\\gamma$ and noise $\\eta$, the agent would tend to choose the close exit and would even risk the cliff, which is the dotted path in the given image.\n",
    "\n",
    "### Case #2: $\\gamma = 0.1$, $\\eta = 0.5$ (low discount factor, high noise)\n",
    "Now, the discount factor is still small, but the environment is highly stochastic. The agent would still be short-sighted but now it would not risk falling down the cliff and would therefore take the long path, that is the solid line to the state with reward **$+1$**, it wouldn't take the state with reward **$+10$** again because the overall incentive is lower. Therefore, for low discount factor $\\gamma$ but high noise $\\eta$ the agent would tend to choose the close exit but not risk falling down the cliff, which is the path denoted by solid line.\n",
    "\n",
    "### Case #3: $\\gamma = 0.9$, $\\eta = 0$ (high discount factor, low noise)\n",
    "The discount factor is large, but there is no noise in the environment. The agent would choose a higher reward even if it comes later down the line. In other words, the agent is far-sighted. The agent would tend to choose the distant exit because of the higher reward associated with it and would risk the cliff because the environment is deterministic. In other words, it would choose the dotted line to the state with reward **$+10$**\n",
    "\n",
    "### Case #4: $\\gamma = 0.9$, $\\eta = 0.5$ (high discount, high noise)\n",
    "The discount factor is large, and the environment is also stochastic. The agent would still choose a higher reward even if it comes later down the line. The agent is far-sighted. The agent would tend to choose the distant exit because of the higher reward associated with it but it wouldn't risk falling down the cliff. In other words, it would choose the solid line to the state with reward **$+10$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# Question-5\n",
    "## Part-a\n",
    "Yes, it is possible to get $Q^{\\pi}_3(s)$ using $Q^{\\pi}_2(s)$ and $Q^{\\pi}_1(s)$   \n",
    "First we will prove additivity in Reward vectors\n",
    "\\begin{align}\n",
    "R^\\pi(s) &= \\sum_{a \\in A} \\pi(a|s) \\sum_{s' in S} P_{ss'}^aR_{ss'}^a \\\\\n",
    "R_3^\\pi(s) &= \\sum_{a \\in A} \\pi(a|s) \\sum_{s' in S} P_{ss'}^a(R_{1_{ss'}}^a +R_{2_{ss'}}^a) \\\\\n",
    "R_3^\\pi(s) &= R_1^\\pi(s) + R_2^\\pi(s)\n",
    "\\end{align}\n",
    "Now, we will prove that the value functions follow additivity.\n",
    "From the Bellman equation, we know that \n",
    "$$V^\\pi = \\left(I - \\gamma P\\right)^{-1}R^\\pi$$\n",
    "We can write\n",
    "$$V_3^\\pi = \\left(I - \\gamma P^\\pi\\right)^{-1}R_3^\\pi = \\left(I - \\gamma P^\\pi\\right)^{-1}\\left(R_1^\\pi+R_2^\\pi\\right) = V_1^\\pi + V_2^\\pi$$\n",
    "\n",
    "Now, we can use\n",
    "$$Q^\\pi(s,a) = \\sum_{s' \\in S} P_{ss'}^a\\left[\\gamma V^\\pi(s') + R_{ss'}^a\\right]$$\n",
    "\n",
    "Now, we can write it as\n",
    "\\begin{align}\n",
    "Q_3^\\pi(s,a) &= \\sum_{s' \\in S} P_{ss'}^a\\left[\\gamma V_3^\\pi(s') + R_{3_{ss'}}^a\\right] \\\\\n",
    "&= \\sum_{s' \\in S} P_{ss'}^a\\left[\\gamma \\left(V_1^\\pi(s')+V_2^\\pi(s')\\right) + R_{1_{ss'}}^a+R_{2_{ss'}}^a\\right]\\\\\n",
    "&= Q_1^\\pi(s,a)+Q_2^\\pi(s,a)\n",
    "\\end{align}\n",
    "\n",
    "## Part-b\n",
    "We cannot combine the two optimal policies $\\pi_1^*$, $\\pi_2^*$, this is because the action that the optimal policy would take may not come from either of the two policies. Max operator is non-linear and we won't get a simple relation.\n",
    "\n",
    "## Part-c \n",
    "We can use the result from part-a, that\n",
    "$$V^{\\pi}_3 = V^{\\pi}_1 + V^{\\pi}_2$$\n",
    "\n",
    "Now, consider that a policy better than $\\pi^*$ (at least on 1 state) exists for $M_3$, lets call this policy $\\pi^{'*}$, we know\n",
    "\n",
    "$$V^{\\pi^{'*}}_3 = V^{\\pi^{'*}}_2 + V^{\\pi^{'*}}_1$$\n",
    "\n",
    "We also know that \n",
    "$$V^{\\pi^{'*}}_3 \\geq V^{\\pi^{*}}_3$$ \n",
    "since $\\pi^{'*}$ is the optimal policy for $M_3$\n",
    "\n",
    "Therefore at least one of the two must hold\n",
    "1. policy $\\pi^{'*}$ is better than $\\pi^{*}$ for $M_1$\n",
    "2. policy $\\pi^{'*}$ is better than $\\pi^{*}$ for $M_2$\n",
    "\n",
    "But we know that the policy $\\pi^{*}$ is optimal for both $M_1$ and $M_2$ which is a contradiction.     \n",
    "This is can also be seen as follows\n",
    "$$V^{\\pi^{'*}}_3 \\geq V^{\\pi^{*}}_3 = V^{\\pi^{*}}_1+V^{\\pi^{*}}_2 \\geq V^{\\pi^{'*}}_1+V^{\\pi^{'*}}_2$$\n",
    "$\\therefore$ the policy $\\pi^{*}$ is optimal for $M_3$\n",
    "\n",
    "## Part-d\n",
    "FIrst we will find a relation between the reward vectors\n",
    "\n",
    "\\begin{align}\n",
    "R_1^{\\pi}(s) &= \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S}P_{ss'}^aR_{1_{ss'}}^a \\\\\n",
    "&= \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S}P_{ss'}^a(R_{1_{ss'}}^a+\\epsilon)\\\\\n",
    "&= \\sum_{a \\in A} \\pi(a|s) \\sum_{s' \\in S}P_{ss'}^a\\epsilon + R_2^{\\pi}(s)\\\\\n",
    "&= \\epsilon \\sum_{a \\in A} \\pi(a|s) + R_2^{\\pi}(s)\\\\\n",
    "&= \\epsilon + R_2^{\\pi}(s)\n",
    "\\end{align}\n",
    "\n",
    "Now, we can use bellman equation\n",
    "\\begin{align}\n",
    "V_1^{\\pi} &= (I - \\gamma P)^{-1}R_1^\\pi\\\\\n",
    "&= (I - \\gamma P)^{-1}(R_2^\\pi+\\epsilon \\vec{1})\\\\\n",
    "&= V_2^\\pi + \\epsilon (I-\\gamma P)^{-1}\\vec{1}\n",
    "\\end{align}\n",
    "\n",
    "We can simplify this for infinite horizon MDP, assume\n",
    "$$V_1^\\pi = V_2^\\pi + \\vec{k} $$\n",
    "\n",
    "From the results obtained above\n",
    "\\begin{align}\n",
    "k\\vec{1} = \\epsilon (I-\\gamma P)^{-1}\\vec{1}\\\\\n",
    "k(I-\\gamma P)\\vec{1} = \\epsilon \\vec{1}\\\\\n",
    "k(\\vec{1}-\\gamma)\\vec{1} = \\epsilon \\vec{1}\\\\\n",
    "k = \\frac{\\epsilon}{1-\\gamma}\n",
    "\\end{align}\n",
    "Note that this result holds only for infinite horizon MDPs,\n",
    "\n",
    "### Why?\n",
    "Because for finite horizon MDPs, we would have to omit the transition probability from absorbing state to itself. An example is given below for finite horizon MDPs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0.9, 0.1],[0, 1]])\n",
    "R = np.array([1,10])\n",
    "P[-1, -1] = 0\n",
    "V1 = ValueVector(P, R, 0.25)\n",
    "V2 = ValueVector(P, R+0.1, 0.5)\n",
    "print(V1)\n",
    "print(V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finite horizon MDP case doesn't simplify any further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
