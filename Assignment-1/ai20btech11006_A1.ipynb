{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - a\n",
    "The states are as follows\n",
    "$$\\{S,1,7,3,8,5,6,W\\}$$\n",
    "This is because we can club together equivalent states, which are \n",
    "$$\\{2 \\equiv 7\\}, \\{3 \\equiv 9\\}, \\{4 \\equiv 8\\}$$\n",
    "\n",
    "The State transition matrix is stated below\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "                  0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0.25 & 0 & 0.25 & 0.25 & 0.25 & 0 \\\\\n",
    "                  0 & 0 & 0 & 0.25 & 0.5 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0.25 & 0 \\\\\n",
    "                  0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25 \\\\\n",
    "                  0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "Note that the states in transition matrix are in the order of states written above.\n",
    "\n",
    "### Part-b\n",
    "The reward function would be \n",
    "$$\n",
    "R(s) = \\begin{cases}\n",
    "                        0 & \\text{if } s = W \\\\\n",
    "                        -1 & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "$$\n",
    "In vector form it could be represented as\n",
    "$$\n",
    "R = \\begin{pmatrix}\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                -1 \\\\\n",
    "                0\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "The discount factor should be 1, but since the $I - P$ is singular we would take the discount factor very close to 1 instead of exactly 1, we could take something like 1 - 1e-6.\n",
    "\n",
    "*Why?* The reward should be -1 for all the states except for the winning state because our goal is to find the average number of steps to reach the goal state, while our discount factor should be 1 again for the same reason.\n",
    "\n",
    "Now we can use the matrix form derived from the Bellman equation to solve for the value vector\n",
    "\n",
    "$$ V = \\left( I - \\gamma P \\right)^{-1}R $$\n",
    "\n",
    "The Calculation can be found in the cell below.\n",
    "\n",
    "The expected number of die throws should be slightly above 7.08 from the starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Value vector is [-7.08329803 -6.99996533 -5.33330844 -6.66663422 -5.33330844 -6.66663422\n",
      " -5.33330844  0.        ]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "# Reward Vector\n",
    "R = -np.ones(8)\n",
    "R[-1] = 0\n",
    "\n",
    "# transition Matrix\n",
    "P = np.array(\n",
    "            [[0 , 0.25 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0.25 , 0 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0.25 , 0 , 0.25 , 0.25 , 0.25 , 0],\n",
    "             [0 , 0 , 0 , 0.25 , 0.5 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0.25 , 0],\n",
    "             [0 , 0 , 0.25 , 0.25 , 0.25 , 0 , 0 , 0.25],\n",
    "             [0 , 0 , 0 , 0 , 0 , 0 , 0 , 1]]\n",
    ")\n",
    "\n",
    "def ValueVector(P: np.ndarray, R: np.ndarray, gamma: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given transition matrix, reward vector and discount factor this\n",
    "    function calculates the value vector\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        P (np.ndarray): transition matrix\n",
    "        R (np.ndarray): Reward vector\n",
    "        gamma (int): the discount factor\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        np.ndarray: the value vector\n",
    "    \"\"\"\n",
    "    # The identity matrix\n",
    "    I = np.eye(*P.shape)\n",
    "    return scipy.linalg.inv((I - gamma*P)).dot(R)\n",
    "\n",
    "V = ValueVector(P, R, 1-1e-6)\n",
    "print(f'The Value vector is {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Reinforcement-Learning-AI3000-5h7sRkvW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f4467fee1144745404ca5d9a7e846f91cc9580f4cb30541d3281f81d0c5adba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
